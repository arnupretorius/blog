{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - How computers learn - \n",
    "## Part 2: Noise and the normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post, we looked at linear relationships and started asking the question of how a computer might be able to learn such a relationship. Specifically, we constructed a toy linear relationship between the number of cups of water represented by the variable $x$ and the number of centimetres of growth per day $y$, for a watermelon plant (the example is a bit weird I know, but I think plants are pretty cool so I will stick with this one). However, the relationship seemed fairly unrealistic. Firstly, more water always resulted in more growth and the change in growth was exactly the same for every one unit change in water.  Ulitmately, if we are interested in getting computers to learn something, then we would like them to be able to learn aspects of the real world, and the real world is full of uncertainty. \n",
    "\n",
    "To make our example more realistic, we are going to add *noise* to the situation. The source of the noise is unknown, however we might have a guess at various culprits such as the weather (extra rain, humidity, drought), human error (spilling water, not measuring exactly how much water we give) and a host of other reasons for the relationship to be non-deterministic. This means that the growth of a watermelon plant won't be exactly the same given the same amount of water each time round. What is fundamental about this is that we can't solely rely on specific observations (since they might change in the future), instead we have to find a way to somehow distil into the computer a *general* relationship, established from a finite sample, that seems to hold on average.\n",
    "\n",
    "So how can we add uncertainty to our toy example? Well, the mathematics of uncertainty is written in terms of probabilities and we can make use of *probability distributions* to represent the noise in our scenario. A probability distribution is simply a function that tells us, given that we have observed something, how likely observing *that* something really is. For example, say we plant $50$ watermelon plants for $50$ years, then by now we might have a fairly good intuition for the size of a typical watermelon, give or take. Therefore, if someone were to hand us a new watermelon (of a certain size) from our garden in the new year, we would to some extent be able to say how likely it is that a watermelon of the same size would grow again the following year. Now if that watermelon that was just handed to us was *enormous*, or *microscopic*, our intiution would've steered us to the conclusion that this was a rare event and is not likely to happend again. On the other hand, if it looked quite normal, we would say, 'sure, we will get plently more of those next year'. This human iniution isn't some fluke, but instead comes from an internal probability distribution that has been well approximated through repeated observation of the real world. \n",
    "\n",
    "As it happens, there is a particular distribution that is extremely prevalent in this world. It is called the *normal* distribution, also referred to as the Gaussian distribution, which looks as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~arnup/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# required modules\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "# generate data and range of parameters\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "sigma_range = np.arange(0.1, 5, 0.1)\n",
    "\n",
    "# create data dictionary\n",
    "data = [dict(\n",
    "        visible = False,\n",
    "        line=dict(width=3),\n",
    "        name = 'p(x)',\n",
    "        x = x,\n",
    "        y = normal_distribution(x, 0, step)) for step in sigma_range]\n",
    "\n",
    "# set starting data configuration (sigma = 1)\n",
    "data[9]['visible'] = True\n",
    "\n",
    "# for each value of sigma show corresponding data\n",
    "steps = []\n",
    "for i in range(len(data)):\n",
    "    step = dict(\n",
    "        method = 'restyle',\n",
    "        label=str(sigma_range[i]),\n",
    "        args = ['visible', [False] * len(data)],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "# define plotly slider\n",
    "sliders = [dict(\n",
    "    active = 9,\n",
    "    currentvalue = {\"prefix\": 'Standard deviation: '},\n",
    "    pad = {\"t\": 50},\n",
    "    len = 0.7,\n",
    "    steps = steps\n",
    ")]\n",
    "\n",
    "# create layout and plot\n",
    "layout = dict(sliders=sliders, xaxis={'title':'x'}, yaxis={'title':'probability of observing x'})\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename = 'normal-dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In math speak, the normal distribution is defined as\n",
    "\n",
    "\\begin{align}\n",
    "    p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}},\n",
    "\\end{align}\n",
    "\n",
    "where the parameters $\\mu$ and $\\sigma^2$ characterise the shape of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import numpy as np\n",
    "\n",
    "# normal probability distribution function\n",
    "def normal_distribution(x, mu, sigma):\n",
    "    p_x = (1/np.sqrt(2*np.pi*sigma**2))*np.exp((-(x - mu)**2)/(2*sigma**2))\n",
    "    return p_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\mu$ is the mean of the distribution and specifies the location of the peak (the most probable observation), whereas $\\sigma^2$ is responsible for how often and to what extent observations vary from the mean. We usually write $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ to indicate that a random variable $x$ is distributed according to a normal distribution with mean $\\mu$ and variance $\\sigma^2$. The quantity $\\sigma$ is referred as the standard deviation (you can play around with the standard deviation to change the shape of the distribution above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution is crucial for many aspects of learning and the assumption that our data comes from a normal distribution is regularly made in practice. In the next post we will use the normal distribution to sample noise and generate more realistic observations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
